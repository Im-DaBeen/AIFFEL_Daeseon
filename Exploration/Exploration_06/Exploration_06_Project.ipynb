{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c372329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로젝트: ControlNet으로 조건을 준 이미지 생성하기\n",
    "\n",
    "# 1.필요한 라이브러리를 설치\n",
    "\n",
    "# Stable Diffusion 설치\n",
    "!pip install --upgrade -qq git+https://github.com/huggingface/diffusers.git transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 삭제\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d23846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.윤곽선 검출\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionControlNetPipeline\n",
    "from diffusers.utils import load_image \n",
    "\n",
    "# 이미지 불러오기\n",
    "image = load_image(\n",
    "    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
    ")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c55d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opencv를 사용하여 이미지의 윤곽선을 검출\n",
    "\n",
    "import cv2\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "\n",
    "# 이미지를 NumPy 배열로 변환합니다. \n",
    "image = np.array(image)\n",
    "\n",
    "# threshold를 지정합니다. \n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "# 윤곽선을 검출합니다. \n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)  # NumPy 배열을 PIL 이미지로 변환합니다. \n",
    "\n",
    "canny_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb5d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 윤곽선 검출 전처리기를 사용한 모델 파이프라인을 불러오기\n",
    "\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "\n",
    "canny_controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n",
    "canny_pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=canny_controlnet, torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70040259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 윤곽선을 추출한 이미지에 프롬프트를 적용하여 새로운 이미지를 생성\n",
    "from diffusers import UniPCMultistepScheduler\n",
    "canny_pipe.scheduler = UniPCMultistepScheduler.from_config(canny_pipe.scheduler.config)\n",
    "canny_pipe = canny_pipe.to(\"cuda\")\n",
    "\n",
    "# 동일한 이미지를 생성하기 위해 seed를 지정합니다. \n",
    "generator = torch.manual_seed(0)  \n",
    "\n",
    "# 이미지를 생성합니다. \n",
    "canny_image = canny_pipe(\n",
    "    prompt=\"disco dancer with colorful lights\", \n",
    "    num_inference_steps=20, \n",
    "    generator=generator, \n",
    "    image=canny_image\n",
    ").images[0]\n",
    "\n",
    "# 생성된 이미지를 저장합니다. \n",
    "canny_image.save(\"/aiffel/aiffel/canny_image.png\")  \n",
    "\n",
    "# 생성된 이미지를 출력합니다. \n",
    "canny_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e788386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.프롬프트를 작성하고 하이퍼파라미터를 조절하여 이미지를 생성해 보세요. \n",
    "canny_image = canny_pipe(\n",
    "    prompt=\"disco fox with colorful lights\", \n",
    "    controlnet_conditioning_scale=3.0,\n",
    "    num_inference_steps=30, \n",
    "    generator=generator, \n",
    "    image=canny_image\n",
    ").images[0]\n",
    "\n",
    "# 생성된 이미지를 저장합니다. \n",
    "canny_image.save(\"/aiffel/aiffel/canny_image.png\")  \n",
    "\n",
    "# 생성된 이미지를 출력합니다. \n",
    "canny_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.인체 자세 감지\n",
    "# controlnet-aux를 설치합니다. Human pose를 검출해주는 controlnet의 보조용 모델입니다.\n",
    "!pip install controlnet-aux==0.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "\n",
    "openpose_image = load_image(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png\"\n",
    ")\n",
    "openpose_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e86a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Openpose 검출기를 사용하면 자세만 추출\n",
    "from controlnet_aux import OpenposeDetector\n",
    "\n",
    "# 인체의 자세를 검출하는 사전 학습된 ControlNet 불러오기\n",
    "openpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
    "\n",
    "# 이미지에서 자세 검출\n",
    "openpose_image = openpose(openpose_image)\n",
    "openpose_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec414f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Openpose 전처리기를 사용한 모델 파이프라인을 불러오기\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel \n",
    "\n",
    "openpose_controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-openpose\", torch_dtype=torch.float16)\n",
    "openpose_pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=openpose_controlnet, torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UniPCMultistepScheduler\n",
    "\n",
    "openpose_pipe.scheduler = UniPCMultistepScheduler.from_config(openpose_pipe.scheduler.config)\n",
    "openpose_pipe = openpose_pipe.to(\"cuda\")\n",
    "\n",
    "# Q. 코드를 작성해 보세요.\n",
    "# 동일한 이미지를 생성하기 위해 seed를 넣어줍니다. \n",
    "generator = torch.manual_seed(0)\n",
    "\n",
    "#프롬프트를 작성합니다. \n",
    "prompt =  'little girl'\n",
    "negative_prompt =  'sun'\n",
    "\n",
    "images = openpose_image\n",
    "\n",
    "# 이미지를 생성합니다. \n",
    "openpose_image1 = openpose_pipe(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=10,\n",
    "    generator=generator,\n",
    "    image=images\n",
    ").images[0]\n",
    "\n",
    "openpose_image1.save(\"/aiffel/openpose_image.png\")\n",
    "\n",
    "# 생성된 이미지를 출력합니다. \n",
    "openpose_image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af30c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 윤곽선 검출 + 인체 자세 감지\n",
    "# Canny 알고리즘을 사용한 윤곽선 검출\n",
    "from diffusers.utils import load_image \n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "# Q. 코드를 작성해 보세요.\n",
    "# 이미지를 불러오세요. \n",
    "image_path = \"/aiffel/openpose_image.png\"\n",
    "canny_image = Image.open(image_path)\n",
    "\n",
    "#threshhold를 지정합니다. \n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "# 이미지를 NumPy 배열로 변환합니다. \n",
    "canny_image = np.array(canny_image)\n",
    "\n",
    "# 인체 감지 포즈를 넣어줄 가운데 부분을 지워줍니다. \n",
    "zero_start = canny_image.shape[1] // 4\n",
    "zero_end = zero_start + canny_image.shape[1] // 2\n",
    "canny_image[:, zero_start:zero_end] = 0\n",
    "\n",
    "# 윤곽선을 검출하고 NumPy 배열을 PIL 이미지로 변환합니다. \n",
    "canny_image = cv2.Canny(canny_image, low_threshold, high_threshold)\n",
    "canny_image = canny_image[:, :, None]\n",
    "canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)\n",
    "canny_image = Image.fromarray(canny_image)\n",
    "\n",
    "canny_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a2a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Openpose를 사용한 인체 자세 검출\n",
    "\n",
    "from controlnet_aux import OpenposeDetector\n",
    "from diffusers.utils import load_image \n",
    "\n",
    "# Q. 아래의 코드를 작성해 주세요.\n",
    "# 이미지를 불러옵니다. \n",
    "image_path = \"/aiffel/openpose_image.png\"\n",
    "openpose_image = Image.open(image_path)\n",
    "\n",
    "# OpenposeDetector를 사용하여 인체 자세를 검출합니다. \n",
    "openpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
    "openpose_image = openpose(openpose_image)\n",
    "\n",
    "openpose_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4df312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler  \n",
    "\n",
    "# Q. 코드를 작성해 보세요.\n",
    "# Edge Detection과 Openpose, 2개의 전처리기를 controlnets라는 리스트로 만듭니다. \n",
    "edge_detection = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n",
    "openpose = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-openpose\", torch_dtype=torch.float16)\n",
    "controlnets = [edge_detection, openpose]\n",
    "\n",
    "# 리스트 controlnets를 파이프라인으로 전달합니다. \n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", controlnet=controlnets, torch_dtype=torch.float16)\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# 프롬프트를 작성합니다. \n",
    "prompt =  'suit man'\n",
    "negative_prompt =  'black'\n",
    "\n",
    "# seed를 지정합니다. \n",
    "generator = torch.manual_seed(0)\n",
    "\n",
    "images = [openpose_image, canny_image]\n",
    "\n",
    "# 이미지를 생성합니다. \n",
    "image = pipe(prompt=prompt, negative_prompt=negative_prompt, generator=generator, image=images).images[0]\n",
    "\n",
    "# 생성된 이미지를 저장합니다.\n",
    "# image.save(\"aiffel/multi_controlnet_output.png\")\n",
    "\n",
    "# 생성된 이미지를 출력합니다.  \n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8611df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"생성한 이미지를 올린 곳은 {https://github.com/style4da/AIFFEL_Quest/tree/master/Exploration/Exploration_06/Exploration_06_Project}입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ecf6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회고\n",
    "\n",
    "# 1. 이미지 생성과 파이프라인을 통해 이미지 특징을 강화해보았는데 하이퍼파라미터를 수정하면서 기준을 잡아 테스트를 진행하는 것이 어려웠습니다.\n",
    "# 2. 코드를 확인하고 짜는 과정이 디테일한 부분에서 놓치는 것들을 수정하면서 잡아나갔습니다.이 부분역시 어려움을 느꼈습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016d21a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
